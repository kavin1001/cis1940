# Laziness

A key feature of Haskell that distinguishes it from other languages, including other functional languages, is that it is _lazy_. Let's talk about what that means.

## Strict evaluation

Before we talk about lazy evaluation, it will be useful to look at some examples of its opposite, _strict evaluation_.

Under a strict evaluation strategy, function arguments are completely evaluated before passing them to the function. For example, suppose we have

```Haskell
f x y = x + 2
```

In a strict language, evaluating `f 5 (29^35792)` will first completely evaluate `5` (already done) and `29^35792` (which is a lot of work) before passing the results to `f`.

Of course, in this particular example, this is silly, since `f` ignores its second argument, so all the work to compute `29^35792` was wasted. So why would we want this?

The benefit of strict evaluation is that it is easy to predict _when_ and _in what order_ things will happen. Usually languages with strict evaluation will even specify the order in which function arguments should be evaluated (e.g. from left to right).

For example, in Java if we write

```Java
f (release_monkeys(), increment_counter())
```

we know that the monkeys will be released, and then the counter will be incremented, and then the results of doing those things will be passed to `f`.

If the releasing of monkeys and incrementing of the counter could independently happen, or not, in either order, depending on whether `f` happens to use their results, it would be extremely confusing. When such "side effects" are allowed, strict evaluation is really what you want.

But Haskell is a very _pure_ language, which lends itself well to laziness:

## Lazy evaluation

Under a _lazy evaluation_ strategy, evaluation of function arguments is delayed as long as possible: they are not evaluated until it actually becomes necessary to do so. When some expression is given as an argument to a function, it is simply packaged up as an unevaluated expression (called a "thunk") without doing any actual work.

For example, when evaluating `f 5 (29^35792)`, the second argument will simply be packaged up into a thunk without doing any actual computation, and `f` will be called immediately. Since `f` never uses its second argument, the thunk will just be thrown away by the garbage collector.

## Pattern matching drives evaluation

So, when is it "necessary" to evaluate an expression? The examples above concentrated on whether a function used its arguments, but this is actually not the most important distinction. Consider the following examples:

```Haskell
f1 :: Maybe a -> [Maybe a]
f1 m = [m, m]

f2 :: Maybe a -> [a]
f2 Nothing  = []
f2 (Just x) = [x]
```

`f1` and `f2` both use their argument. But there is still a big difference between them. Although `f1` uses its argument `m`, it does not need to know anything about it. `m` can remain completely unevaluated, and the unevaluated expression is simply put in a list. Put another way, the result of `f1 e` does not depend on the shape of `e`.

`f2`, on the other hand, needs to know something about its argument in order to proceed: was it constructed with `Nothing` or `Just`? That is, in order to evaluate `f2 e`, we must first evaluate `e`, because the result depends on the shape of `e`.

The other important thing to note is that thunks are evaluated only enough to allow a pattern match to proceed, and no further! For example, suppose we wanted to evaluate `f2 (safeHead [3^500, 49])`. `f2` would force evaluation of the call to `safeHead [3^500, 49]`, which would evaluate to `Just (3^500)`, but the `3^500` is not evaluated, since `safeHead` does not need to look at it, and neither does `f2`. Whether the `3^500` gets evaluated later depends on how the result of `f2` is used.

The slogan to remember is "pattern matching drives evaluation". To reiterate the important points:

-   Expressions are only evaluated when pattern-matched
-   ...as far as necessary for the match to proceed, and no further!

# More examples

## Short-circuiting operators

In some languages (such as Java) the boolean operators `&&` and `||` are short-circuiting: for example, if the first argument to `&&` evaluates to false, the whole expression will immediately evaluate to false without touching the second argument. However, this behavior has to be wired into Java as a special case.

In Haskell, however, we can define short-circuiting operators without any special handling. In fact, `(&&)` and `(||)` are just plain library functions! Hereâ€™s how `(&&)` is defined:

```Haskell
(&&) :: Bool -> Bool -> Bool
True  && x = x
False && _ = False
```

Notice how this definition of `(&&)` does not pattern-match on its second argument. Consider this alternative:

```Haskell
(&&!) :: Bool -> Bool -> Bool
True  &&! True  = True
True  &&! False = False
False &&! True  = False
False &&! False = False
```

While this version takes on the same values as `(&&)`, it has different behavior. For example, compare the following:

```Haskell
False &&  (34^9784346 > 34987345)
False &&! (34^9784346 > 34987345)
```

These will both evaluate to `False`, but the second one will take a lot longer! Or how about this:

```Haskell
False &&  (head [] == 'x')
False &&! (head [] == 'x')
```

The first one is again `False`, whereas the second one will crash. Try it!

## Infinite data structures

Lazy evaluation also means that we can work with infinite data structures. For example, consider

```Haskell
repeat :: a -> [a]
repeat x = x : repeat x

take :: Int -> [a] -> [a]
take n _ | n <= 0 = []
take _ [] = []
take n (x : xs) = x : take (n - 1) xs
```

`repeat 7` will produce an infinite list of 7s. What happens if we have `take 3 (repeat 7)`? Because of the way `take` is defined, we do not need to evaluate the entire result of `repeat 7` (which would take, literally, forever) and can instead immediately return `[7, 7, 7]`.

We can also define an infinite list of natural numbers like so:

```Haskell
nats :: [Int]
nats = 0 : map (+ 1) nats
```

What would `take 3 nats` evaluate to?

## Understanding complexity

One drawback of laziness is that it becomes harder to reason about the complexity of Haskell programs. (I think this will be discussed in class!)
